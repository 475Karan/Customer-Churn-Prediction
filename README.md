# Customer-Churn-Prediction

Project 2: Customer Churn Prediction
Goal:
Identify customers likely to leave a service or subscription.
Dataset:
Name: Telco Customer Churn dataset
Source: This dataset is often available on Kaggle. It contains customer information from a telecommunications company, including usage, service subscriptions, and account details, along with whether or not they churned.
Data Preprocessing:
Data Loading: The dataset was loaded from a CSV file using pandas.
Data Cleaning:
The 'TotalCharges' column was converted to numeric, with errors coerced to NaN (Not a Number) to handle invalid entries.
Rows with missing values (NaN in 'TotalCharges') were dropped.
Categorical Feature Encoding:
Categorical features were encoded using LabelEncoder to convert them into numerical representations. The 'customerID' column was excluded from this process.
Feature and Target Definition:
The features (input variables) were separated from the target variable ('Churn'). 'customerID' was also excluded from the features.
Data Splitting:
The data was split into training and testing sets (80% train, 20% test) using train_test_split with a random_state of 42 for reproducibility.
Handling Imbalanced Data:
SMOTE (Synthetic Minority Over-sampling Technique) was applied to the training set to address the class imbalance issue, creating synthetic samples of the minority class (churned customers).
Numerical Feature Scaling:
Numerical features were scaled using StandardScaler to have a mean of 0 and a standard deviation of 1. This was done separately for the training and test sets to prevent data leakage.
Models Used:
Random Forest:
A Random Forest Classifier was used to predict customer churn.
Key parameters (example - include the parameters you used or found through grid search):
n_estimators: 100 (or as tuned)
max_depth: None (or as tuned)
random_state: 42
Logistic Regression
Logistic Regression was used as a comparative baseline model.
Key parameters:
random_state: 42
solver: 'liblinear'
class_weight: 'balanced'
Decision Tree Classifier
A Decision Tree Classifier was used as another comparative model.
Key Parameters
random_state: 42
Evaluation Metrics:
Confusion Matrix: Visualizes the number of correct and incorrect predictions for each class (churned and not churned).
Classification Report: Provides precision, recall, and F1-score for each class, offering a detailed evaluation of the model's performance.
AUC-ROC: (Area Under the Receiver Operating Characteristic Curve): Measures the model's ability to distinguish between customers who will churn and those who will not.
Precision-Recall Curve: Plots precision against recall at various threshold settings.
AUC-PR: (Area Under the Precision-Recall Curve): Summarizes the trade-off between precision and recall.
Feature Importance: (For Random Forest) Quantifies the importance of each feature in the model's predictions.
Results:
A comparison of the models' performance on the test set is as follows (these are example results, replace with your actual numbers):
Model
Accuracy
Precision (Churn)
Recall (Churn)
F1-Score (Churn)
AUC-ROC
AUC-PR
Random Forest
0.80
0.65
0.55
0.60
0.85
0.70
Logistic Regression
0.75
0.50
0.70
0.58
0.80
0.60
Decision Tree
0.73
0.48
0.50
0.49
0.70
0.55

The Random Forest model generally outperformed the other models, achieving a good balance of precision and recall, as reflected in its F1-score and AUC-PR. The Logistic Regression model had a higher recall for churn but lower precision. The Decision Tree performed the worst. (Include the actual confusion matrices, ROC curves, precision-recall curves, and feature importance plots generated by your code in your final document).
The feature importance plot from Random Forest shows that the following features were most predictive of churn (list top 5 or so):
Feature 1: e.g., TotalCharges
Feature 2: e.g., MonthlyCharges
Feature 3: e.g., Tenure
Feature 4: e.g., Contract
Feature 5: e.g., OnlineSecurity
Conclusion:
The Random Forest model was more effective in predicting customer churn than Logistic Regression and Decision Tree, likely due to its ability to capture non-linear relationships and feature interactions.
Key factors influencing churn, as identified by Random Forest's feature importance, include [Summarize the most important features].
Potential next steps include:
Further hyperparameter tuning of the Random Forest model (or the best-performing model).
Exploring other advanced techniques for handling imbalanced data.
Analyzing the reasons behind the importance of the key features.
Developing targeted retention strategies based on the model's predictions and feature importance.
Evaluating the cost-effectiveness of different retention strategies.
